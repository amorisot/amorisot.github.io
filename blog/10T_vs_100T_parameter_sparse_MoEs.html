<!doctype html>
<html>
  <head>
    <link rel="icon" type="image/png" href="/favicon.png"/>
    <link rel="stylesheet" href="/static/style.css">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="As of September 2025, LLMs are spikily impressive: they outperform most people in mathematical and coding abilities, and they speak more languages fluently...">
    <meta property="og:title" content="10T vs 100T parameter sparse MoE's - Adrien Morisot">
    <meta property="og:description" content="As of September 2025, LLMs are spikily impressive: they outperform most people in mathematical and coding abilities, and they speak more languages fluently...">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://amorisot.github.io/blog/10T_vs_100T_parameter_sparse_MoEs.html">
    <link rel="canonical" href="https://amorisot.github.io/blog/10T_vs_100T_parameter_sparse_MoEs.html">
    <title>Blog - 10T vs 100T parameter sparse MoE's</title>
    <script type="application/ld+json">{"@context": "https://schema.org", "@type": "BlogPosting", "headline": "10T vs 100T parameter sparse MoE's", "datePublished": "2025-09-11", "dateModified": "2025-12-23"}</script>
  </head>
  <body>
    <div id="menu">
      <li><a href="/about">ğŸ¡</a></li>
      <li><a href="/art">ğŸ‘¨â€ğŸ¨</a></li>
      <li><a href="/ml">ğŸ¤–</a></li>
      <li><a href="/books">ğŸ“š</a></li>
      <li><a href="/blog">ğŸ“</a></li>
    </div>
    <div id="left">
      &nbsp;
    </div>
    <div id="content">
      <h3>10T vs 100T parameter sparse MoE's</h3>
      <p class="post-dates"><small>Published: Sep 11, 2025 Â· Updated: Dec 23, 2025</small></p>
      <p>As of September 2025, LLMs are spikily impressive: they outperform most people in mathematical and coding abilities, and they speak more languages fluently than any one person.</p>
      <p>The depth and breadth of even small LLM's knowledge is staggering. However, it is important to remember that their competition is stiff, when it comes to performing tasks in the world.</p>
      <p>Current generation LLMs --at most ~10T parameter sparse MoEs-- are competing against the human brain, which are 100T parameter, highly efficient, multimodal MoEs, pretrained on decades of experience, post-trained in university, RLVR'ed/RLHF'ed on the job. Humans should not be slept on.</p>
    </div>
    <script data-goatcounter="https://amorisot.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
  </body>
</html>